{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVkCC1iri2SN"
   },
   "source": [
    "## HW 4: Policy gradient\n",
    "_Reference: based on Practical RL course by YSDA_\n",
    "\n",
    "In this notebook you have to master Policy gradient Q-learning and apply it to familiar (and not so familiar) RL problems once again.\n",
    "\n",
    "To get used to `gymnasium` package, please, refer to the [documentation](https://gymnasium.farama.org/introduction/basic_usage/).\n",
    "\n",
    "\n",
    "In the end of the notebook, please, copy the functions you have implemented to the template file and submit it to the Contest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7UYczVTli2Sb"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "XPKYrIlai2Sf",
    "outputId": "2e044ee7-3baa-4bd7-a214-23b7225a88b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\PycharmProjects\\YandexMLTrain3\\.venv\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a511eb4b00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75eHkuwTi2Si"
   },
   "source": [
    "# Building the network for Policy Gradient (REINFORCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_TFCmsWi2Sj"
   },
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sY2THBWfi2Sl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8_pYr7PZi2Sn"
   },
   "outputs": [],
   "source": [
    "# Build a simple neural network that predicts policy logits.\n",
    "# Keep it simple: CartPole isn't worth deep architectures.\n",
    "model = model = nn.Sequential(nn.Linear(4, 128), nn.ReLU(), nn.Linear(128, 2))\n",
    "assert model is not None, \"model is not defined\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Y80qbQFi2Sq"
   },
   "source": [
    "#### Predicting the action probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12PjRu0mi2Sr"
   },
   "source": [
    "Note: **output value of this function is not a torch tensor, it's a numpy array.**\n",
    "\n",
    "So, here gradient calculation is not needed.\n",
    "\n",
    "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
    "to suppress gradient calculation.\n",
    "\n",
    "Also, `.detach()` can be used instead, but there is a difference:\n",
    "\n",
    "* With `.detach()` computational graph is built but then disconnected from a particular tensor, so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
    "* In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "d5B5JuXCi2St"
   },
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\"\n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # convert states, compute logits, use softmax to get probability\n",
    "\n",
    "    # YOUR CODE GOES HERE\n",
    "    with torch.no_grad():  # suppress gradient calculation\n",
    "        # Переводим массив в тензор\n",
    "        states_tensor = torch.FloatTensor(states)\n",
    "        \n",
    "        # Получим необраболтанные выходы нейросети\n",
    "        logits = model(states_tensor)\n",
    "        \n",
    "        # Применим к выходам софтмакс, для получения вероятностей классов\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        # переведем полученный тензор в массив\n",
    "        probs = probs.numpy()\n",
    "    assert probs is not None, \"probs is not defined\"\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Obkl_jCii2Sv"
   },
   "outputs": [],
   "source": [
    "test_states = np.array([env.reset()[0] for _ in range(5)])\n",
    "test_probas = predict_probs(test_states)\n",
    "assert isinstance(test_probas, np.ndarray), \\\n",
    "    \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
    "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Be6AYf8gi2Sw"
   },
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8LOUUvnki2Sx"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=1000):\n",
    "    \"\"\"\n",
    "    Play a full session with REINFORCE agent.\n",
    "    Returns sequences of states, actions, and rewards.\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s, info = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(np.array([s]))[0]\n",
    "\n",
    "        # Sample action with given probabilities.\n",
    "        a = np.random.choice(n_actions, p=action_probs)\n",
    "        new_s, r, done, truncated, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5sdENWJAi2Sz"
   },
   "outputs": [],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eG5hLg-3i2S0"
   },
   "source": [
    "### Computing cumulative rewards\n",
    "\n",
    "To work with sequential environments we need the cumulative discounted reward for known for every state. To compute it we can **roll back** from the end of the session to the beginning and compute the discounted cumulative reward as following:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
    "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
    "&= r_t + \\gamma * G_{t + 1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "AoWX9gvai2S0"
   },
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Take a list of immediate rewards r(s,a) for the whole session\n",
    "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
    "\n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    A simple way to compute cumulative rewards is to iterate from the last\n",
    "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    # YOUR CODE GOES HERE\n",
    "    cumulative_rewards = []\n",
    "    G = 0\n",
    "    \n",
    "    # будем итерировать с конца списка к началу\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        cumulative_rewards.append(G)        \n",
    "    \n",
    "    cumulative_rewards = list(reversed(cumulative_rewards))\n",
    "    assert cumulative_rewards is not None, \"cumulative_rewards is not defined\"\n",
    "\n",
    "    return cumulative_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DX39wcUi2S3",
    "outputId": "9916590d-b093-4c5b-dd84-4ed9ed4b2ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
    "    [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evLt5DJji2S_"
   },
   "source": [
    "### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
    "\n",
    "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
    "\n",
    "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
    "\n",
    "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient.\n",
    "\n",
    "Final loss should also include the entropy regularization term $H(\\pi_\\theta (a_i \\mid s_i))$ to enforce the exploration:\n",
    "\n",
    "$$\n",
    "L = -\\hat J(\\theta) - \\lambda H(\\pi_\\theta (a_i \\mid s_i)),\n",
    "$$\n",
    "where $\\lambda$ is the `entropy_coef`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function might be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_hLjxTVLi2TB"
   },
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, ndims):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    y_one_hot = torch.zeros(\n",
    "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Compute the loss for the REINFORCE algorithm.\n",
    "    \"\"\"\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int32)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    logits = model(states)\n",
    "    assert logits is not None, \"logits is not defined\"\n",
    "\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    assert probs is not None, \"probs is not defined\"\n",
    "\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    assert log_probs is not None, \"log_probs is not defined\"\n",
    "\n",
    "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    actions_one_hot = to_one_hot(actions, ndims=probs.shape[-1])\n",
    "    log_probs_for_actions = (log_probs * actions_one_hot).sum(dim=-1) # [batch,]\n",
    "    assert log_probs_for_actions is not None, \"log_probs_for_actions is not defined\"\n",
    "    J_hat = (log_probs_for_actions * cumulative_returns).mean()  # a number\n",
    "    assert J_hat is not None, \"J_hat is not defined\"\n",
    "    \n",
    "    # Compute loss here. Don't forget entropy regularization with `entropy_coef`\n",
    "    entropy = - (probs * log_probs).sum(dim=-1).mean()\n",
    "    assert entropy is not None, \"entropy is not defined\"\n",
    "    loss = -J_hat - entropy_coef * entropy\n",
    "    assert loss is not None, \"loss is not defined\"\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1C8ZSizji2TD"
   },
   "outputs": [],
   "source": [
    "# Your code: define optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "\n",
    "def train_on_session(states, actions, rewards, optimizer=optimizer, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    loss = get_loss(states, actions, rewards, gamma, entropy_coef)\n",
    "\n",
    "    # Gradient descent step\n",
    "\n",
    "    # Optimizer step\n",
    "\n",
    "    # Zero gradients step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-WWsbl5i2TE"
   },
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ckHj5sXBi2TE",
    "outputId": "017d3bcd-0d01-4632-ed1c-60642792cddf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_32448\\2227116181.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
      "  states = torch.tensor(states, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:21.970\n",
      "mean reward:35.030\n",
      "mean reward:44.240\n",
      "mean reward:47.310\n",
      "mean reward:126.880\n",
      "mean reward:184.030\n",
      "mean reward:221.590\n",
      "mean reward:264.430\n",
      "mean reward:158.300\n",
      "mean reward:308.420\n",
      "mean reward:255.690\n",
      "mean reward:205.860\n",
      "mean reward:445.070\n",
      "mean reward:341.040\n",
      "mean reward:187.200\n",
      "mean reward:325.590\n",
      "mean reward:217.230\n",
      "mean reward:316.390\n",
      "mean reward:183.840\n",
      "mean reward:86.900\n",
      "mean reward:131.690\n",
      "mean reward:133.840\n",
      "mean reward:144.930\n",
      "mean reward:125.010\n",
      "mean reward:136.610\n",
      "mean reward:307.200\n",
      "mean reward:318.510\n",
      "mean reward:162.360\n",
      "mean reward:209.280\n",
      "mean reward:114.100\n",
      "mean reward:119.350\n",
      "mean reward:90.830\n",
      "mean reward:99.810\n",
      "mean reward:269.830\n",
      "mean reward:303.250\n",
      "mean reward:115.630\n",
      "mean reward:87.450\n",
      "mean reward:108.560\n",
      "mean reward:117.820\n",
      "mean reward:130.400\n",
      "mean reward:130.090\n",
      "mean reward:130.720\n",
      "mean reward:263.120\n",
      "mean reward:901.330\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    rewards = [train_on_session(*generate_session(env), entropy_coef=1e-3) for _ in range(100)]  # generate new sessions\n",
    "\n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 800:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bg__sQeti2TF"
   },
   "source": [
    "### Watch the video of your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "env_for_video = gym.make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
    "n_actions = env_for_video.action_space.n\n",
    "\n",
    "episode_index = 0\n",
    "step_starting_index = 0\n",
    "\n",
    "obs, info = env_for_video.reset()\n",
    "\n",
    "for step_index in range(800):\n",
    "    probs = predict_probs(np.array([obs]))[0]\n",
    "    action = np.random.choice(n_actions, p=probs)\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env_for_video.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if done or step_index == 799:\n",
    "        # env_for_video.render() now returns the LIST of frames accumulated so far\n",
    "        frames = env_for_video.render()\n",
    "        os.makedirs(\"videos\", exist_ok=True)\n",
    "        save_video(\n",
    "            frames, \"videos\",\n",
    "            fps=env_for_video.metadata.get(\"render_fps\", 30),\n",
    "            step_starting_index=step_starting_index,\n",
    "            episode_index=episode_index,\n",
    "        )\n",
    "        episode_index += 1\n",
    "        step_starting_index = step_index + 1\n",
    "        obs, info = env_for_video.reset()\n",
    "\n",
    "env_for_video.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Finally, copy the `predict_probs`, `get_cumulative_rewards` and `get_loss` to the template and submit them to the Contest.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus part (no points, just for the interested ones)\n",
    "\n",
    "Try solving the `Acrobot-v1` environment. It is more complex than regular `CartPole-v1`, so the default Policy Gradient (REINFORCE) algorithm might not work. Maybe the baseline idea could help...\n",
    "\n",
    "![Acrobot](https://gymnasium.farama.org/_images/acrobot.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your brave and victorious code here."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
